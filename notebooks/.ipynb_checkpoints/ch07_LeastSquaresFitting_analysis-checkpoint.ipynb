{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content:\n",
    "1. [Goodness of a fit: Error metrics](#1.-Goodness-of-a-fit:-Error-metrics)    \n",
    "1.1 [Mean absolute error and standard deviation](##1.1-Mean-absolute-error-and-standard-deviation)    \n",
    "1.2 [Pearson correlation coefficient and scatterplot](##-1.2-Pearson-correlation-coefficient-and-scatterplot)    \n",
    "2. [Weighted least squares ](#2.-Weighted-least-squares)\n",
    "3. [Multiple least squares: Higher dimensional fit ](#3.-Multiple-least-squares:-Higher-dimensional-fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Goodness of a fit: Error metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will learn about how to load data (i.e., x and y values) from a file, line-by-line and then perform least squares fitting. In the following exercise, we are going to fit the data to a cubic polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/fit_01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-70ec5dca0f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#=== Read the data from a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/fit_01.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/fit_01.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#=== Read the data from a file\n",
    "myfile=open('../data/fit_01.csv','r')\n",
    "\n",
    "x=np.array([])\n",
    "y=np.array([])\n",
    "\n",
    "iline=0\n",
    "for line in myfile:\n",
    "    if iline > 0:            # i == 0 corresponds to the heading\n",
    "        str=line.split(',')  # csv, comma separated values\n",
    "        valx=eval(str[0])\n",
    "        valy=eval(str[1])\n",
    "        x=np.append(x,[valx])\n",
    "        y=np.append(y,[valy])\n",
    "        \n",
    "    iline=iline+1\n",
    "myfile.close() \n",
    "    \n",
    "#=== Let's use numpy's polyval and polyfit \n",
    "\n",
    "D=3\n",
    "N=x.shape[0]\n",
    "\n",
    "print('#    i','     a_i')\n",
    "a=np.polyfit(x,y,D)\n",
    "for i in range(D+1):\n",
    "    print('{:6d}{:15.8f}'.format(i,a[i]))\n",
    "\n",
    "p = np.poly1d(a)\n",
    "print(\"\\nThe fitted polynomial is \\n\",p,\"\\n\\n\")\n",
    "\n",
    "yfit=np.polyval(a,x)\n",
    "\n",
    "err=y-yfit\n",
    "abs_err = np.abs(y-yfit)\n",
    "perc_err=np.abs( abs_err/y ) *100\n",
    "\n",
    "print('#    i','     x_i           y_i             y_i (fit)      error          |error|      % error')\n",
    "for i in range(N):\n",
    "    print('{:6d}{:15.8f}{:15.8f}{:15.8f}{:15.8f}{:15.8f}{:15.8f}'.format(i,x[i],y[i],yfit[i],err[i],abs_err[i],perc_err[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the actual data against the fitted function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x,y,'o',color='r',linewidth=1,label='y$^{exact}$')\n",
    "plt.plot(x,yfit,'x',color='b',linewidth=1)\n",
    "\n",
    "xgrids=np.linspace(0,7, 51)\n",
    "ygridsfit=np.polyval(a,xgrids)\n",
    "\n",
    "plt.plot(xgrids,ygridsfit,'-',color='b',linewidth=1,label='y$^{fitted}$')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title('Polynomial fitting')\n",
    "\n",
    "#plt.savefig('test.png')  \n",
    "\n",
    "#=== display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Mean absolute error and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a bit of statistics on the fitted polynomial and see if we have done a good job. The simplest error metrics to look at are \n",
    "\n",
    "\n",
    "\n",
    "Mean absolute error:\n",
    "$$\n",
    "{\\rm Mean\\,absolute\\,error} = \\langle | {\\bf y}-{\\bf y}^{\\rm fitted} | \\rangle=\\frac{\\sum_k | y_k-y_k^{\\rm fitted}|}{N}\n",
    "$$\n",
    "\n",
    "Standard deviation:\n",
    "To define standard deviation, we have to centralize the deviation, i.e., the mean of the error must be zero. So, let's define the mean error as \n",
    "$$\n",
    "{\\rm Mean\\,error} = \\mu = \\langle {\\bf y}-{\\bf y}^{\\rm fitted}\\rangle=\\frac{\\sum_k \\left( y_k-y_k^{\\rm fitted}\\right)}{N}\n",
    "$$\n",
    "$$\n",
    "{\\rm Standard\\,deviation} = \\sqrt{ \\langle ( {\\bf y}-{\\bf y}^{\\rm fitted}-\\mu )^2 \\rangle} =\\sqrt{\\frac{\\sum_k ( y_k-y_k^{\\rm fitted}-\\mu)^2}{N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of data points, N: ', N)\n",
    "print('Degree of the polynomial, D: ', D)\n",
    "\n",
    "#Mean absolute error\n",
    "mae=np.mean(abs_err)\n",
    "print(\"Mean absolute error: \", mae)\n",
    "\n",
    "# Centralized error\n",
    "cerr=err-np.mean(err)\n",
    "\n",
    "# Variance\n",
    "var=np.mean(cerr**2)\n",
    "\n",
    "# Standard deviation\n",
    "std=np.sqrt(var)\n",
    "print(\"Standard deviation:  \", std)\n",
    "\n",
    "# Numpy's standard deviation\n",
    "std=np.std(err)\n",
    "print(\"Standard deviation using numpy: \", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a new concept which we will discuss without a lot of digression. The standard deviation reported above should be thought of as a lower bound. This is because the formula used for standard deviation does not consider the degree of the polynomial. The general idea is that if we use a polynomial with a higher degree (a more flexible function), we should expect a larger standard deviation. Hence, when using polynomial regression, in the denominator of mean, we will use $N-D-1$ instead of $N$. This error will now provide a safe upper bound. \n",
    "$$\n",
    "{\\rm Standard\\,deviation\\,with\\,penalty\\,(for\\,polynomials)} = \\sqrt{ \\langle ( {\\bf y}-{\\bf y}^{\\rm fitted}-\\mu )^2 \\rangle} =\\sqrt{\\frac{\\sum_k ( y_k-y_k^{\\rm fitted}-\\mu)^2}{N-D-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=np.sum(cerr**2)/(N-D-1.0)\n",
    "std=np.sqrt(var)\n",
    "print(\"Standard deviation of polynomial fit [upper bound]: \", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually a good practice to include the error metrics in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x,y,'o',color='r',linewidth=1,label='y$^{exact}$')\n",
    "plt.plot(x,yfit,'x',color='b',linewidth=1)\n",
    "\n",
    "xgrids=np.linspace(0,7, 51)\n",
    "ygridsfit=np.polyval(a,xgrids)\n",
    "\n",
    "plt.text(1,-5, r'Mean error = {0:5.3f}'.format(mae)+' $\\pm$ {0:5.3f}'.format(std), fontsize=12)\n",
    "\n",
    "plt.plot(xgrids,ygridsfit,'-',color='b',linewidth=1,label='y$^{fitted}$')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title('Polynomial fitting')\n",
    "\n",
    "#plt.savefig('test.png')  \n",
    "\n",
    "#=== display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pearson correlation coefficient and scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important error metric is the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) which quantifies the linear correlation between two sets of data. This can be calculated using the numpy's function `corrcoef`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rho = np.corrcoef(y, yfit)[0,1]  #check what is [0,1]\n",
    "print('Pearson correlation coefficient is: ', my_rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scatterplot (which must always be a square plot), one plots the actual y against the fitted-y. Here, it is easy to spot the deviation from the $y=x$ line. The correlation coefficient calculated above maybe provided in a scatterplot as an additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure()                          # comment if square plot is not needed\n",
    "ax = fig.add_subplot(111)                   # comment if square plot is not needed\n",
    "plt.plot(y,yfit,'x',color='blue')\n",
    "ax.set_aspect('equal', adjustable='box')    # comment if square plot is not needed\n",
    "plt.plot(y,y,'-',color='black')\n",
    "\n",
    "plt.text(10,-5, r'$\\rho=$ {0:5.3f}'.format(my_rho), fontsize=10)\n",
    "\n",
    "plt.legend(['fit','$y=x$'])\n",
    "\n",
    "plt.xlabel(\"y$^{exact}$\")\n",
    "plt.ylabel(\"y$^{fitted}$\")\n",
    "plt.title('Scatterplot')\n",
    "\n",
    "#plt.savefig('test.png')  \n",
    "\n",
    "#=== display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Homework-10: Use the data given in '../data/fit_01.csv' and perform least-squares regression using polynomials of degrees $D=1, 2, 3, ..., 8$. For each case, calculate the standard deviation for the polynomial fit. Comment on the polynomial which gives the lowest standard deviation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework-11: Kepler's third law states that the period of a planet's orbit, $P$, and its semimajor axis, $a$, have the relation \n",
    "$$\n",
    "P^2 = a^3\n",
    "$$\n",
    "Here is some data that can be used to check this law. \n",
    "\n",
    "```\n",
    "Planet     P [years]       a [AU]    \n",
    "Mercury    0.242           0.388      \n",
    "Venus      0.616           0.724    \n",
    "Earth      1.000           1.000    \n",
    "Mars       1.881           1.524     \n",
    "Jupiter   11.86            5.200    \n",
    "Saturn    29.33            9.510    \n",
    "```\n",
    "Using least-squares regression, fit the data to the equation $P=a^n$ by first taking the logarithm ${\\rm ln}(P)=n{\\rm ln}(a)$. Discuss your results and comment on the accuracy of the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weighted least squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boardwork025.jpg](../boardwork/boardwork025.jpg)\n",
    "![boardwork026.jpg](../boardwork/boardwork026.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Homework-12: Use the data given in 'data/fit_02.csv' and perform weighted least-squares regression using a polynomials of degrees $D=3$. Column three of the file contains the standard deviation of each measured value of 'y'.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple least squares: Higher dimensional fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boardwork027.jpg](../boardwork/boardwork027.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
